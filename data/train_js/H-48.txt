the effectiveness of information retrieval (ir) systems is 
influenced by the degree of term overlap between user queries
and relevant documents. query-document term mismatch,
whether partial or total, is a fact that must be dealt with by
ir systems. query expansion (qe) is one method for 
dealing with term mismatch. ir systems implementing query
expansion are typically evaluated by executing each query
twice, with and without query expansion, and then 
comparing the two result sets. while this measures an overall
change in performance, it does not directly measure the 
effectiveness of ir systems in overcoming the inherent issue of
term mismatch between the query and relevant documents,
nor does it provide any insight into how such systems would
behave in the presence of query-document term mismatch.
in this paper, we propose a new approach for evaluating
query expansion techniques. the proposed approach is 
attractive because it provides an estimate of system 
performance under varying degrees of query-document term 
mismatch, it makes use of readily available test collections, and
it does not require any additional relevance judgments or
any form of manual processing.
