evaluation of information retrieval systems is one of the core
tasks in information retrieval. problems include the 
inability to exhaustively label all documents for a topic, 
nongeneralizability from a small number of topics, and 
incorporating the variability of retrieval systems. previous work
addresses the evaluation of systems, the ranking of queries
by difficulty, and the ranking of individual retrievals by 
performance. approaches exist for the case of few and even no
relevance judgments. our focus is on zero-judgment 
performance prediction of individual retrievals.
one common shortcoming of previous techniques is the 
assumption of uncorrelated document scores and judgments.
if documents are embedded in a high-dimensional space (as
they often are), we can apply techniques from spatial data
analysis to detect correlations between document scores.
we find that the low correlation between scores of 
topically close documents often implies a poor retrieval 
performance. when compared to a state of the art baseline,
we demonstrate that the spatial analysis of retrieval scores
provides significantly better prediction performance. these
new predictors can also be incorporated with classic 
predictors to improve performance further. we also describe
the first large-scale experiment to evaluate zero-judgment
performance prediction for a massive number of retrieval
systems over a variety of collections in several languages.
