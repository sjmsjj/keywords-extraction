an ever-increasing amount of information on the web today is
available only through search interfaces: the users have to type in a
set of keywords in a search form in order to access the pages from
certain web sites. these pages are often referred to as the hidden
web or the deep web. since there are no static links to the hidden
web pages, search engines cannot discover and index such pages
and thus do not return them in the results. however, according to
recent studies, the content provided by many hidden web sites is
often of very high quality and can be extremely valuable to many
users.
in this paper, we study how we can build an effective hidden web
crawler that can autonomously discover and download pages from
the hidden web. since the only entry point to a hidden web site
is a query interface, the main challenge that a hidden web crawler
has to face is how to automatically generate meaningful queries to
issue to the site. here, we provide a theoretical framework to 
investigate the query generation problem for the hidden web and we
propose effective policies for generating queries automatically. our
policies proceed iteratively, issuing a different query in every 
iteration. we experimentally evaluate the effectiveness of these policies
on 4 real hidden web sites and our results are very promising. for
instance, in one experiment, one of our policies downloaded more
than 90% of a hidden web site (that contains 14 million 
documents) after issuing fewer than 100 queries.
