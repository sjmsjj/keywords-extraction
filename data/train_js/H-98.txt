text classifiers that give probability estimates are more readily 
applicable in a variety of scenarios. for example, rather than 
choosing one set decision threshold, they can be used in a bayesian
risk model to issue a run-time decision which minimizes a 
userspecified cost function dynamically chosen at prediction time. 
however, the quality of the probability estimates is crucial. we review a
variety of standard approaches to converting scores (and poor 
probability estimates) from text classifiers to high quality estimates and
introduce new models motivated by the intuition that the empirical
score distribution for the extremely irrelevant, hard to 
discriminate, and obviously relevant items are often significantly 
different. finally, we analyze the experimental performance of these
models over the outputs of two text classifiers. the analysis 
demonstrates that one of these models is theoretically attractive 
(introducing few new parameters while increasing flexibility), 
computationally efficient, and empirically preferable.
