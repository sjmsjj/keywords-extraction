existing pseudo-relevance feedback methods typically 
perform averaging over the top-retrieved documents, but 
ignore an important statistical dimension: the risk or variance
associated with either the individual document models, or
their combination. treating the baseline feedback method
as a black box, and the output feedback model as a random
variable, we estimate a posterior distribution for the 
feedback model by resampling a given query"s top-retrieved 
documents, using the posterior mean or mode as the enhanced
feedback model. we then perform model combination over
several enhanced models, each based on a slightly modified
query sampled from the original query. we find that 
resampling documents helps increase individual feedback model
precision by removing noise terms, while sampling from the
query improves robustness (worst-case performance) by 
emphasizing terms related to multiple query aspects. the 
result is a meta-feedback algorithm that is both more robust
and more precise than the original strong baseline method.
